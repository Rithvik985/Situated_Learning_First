# If using local vLLM OpenAI-compatible server
VLLM_URL_FOR_ANALYSIS=http://localhost:9091/v1/chat/completions
VLLM_MODEL_FOR_ANALYSIS=ibnzterrell/Meta-Llama-3.3-70B-Instruct-AWQ-INT4